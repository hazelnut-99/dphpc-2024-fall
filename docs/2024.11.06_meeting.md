# 2024-11-06 Meeting Notes

- More fine-grained overlapping
- Alpa - different DP/TP for different operators
- difference between attentionand MLP
	- MLP: mul with weight matrix
	- attention: mul with transient (FSDP)
	- No pipeline/sharding parallism
	- start with sequence parallism
- difference of MLP between GPT and llama
	- GPT: 2 weight matrices fro upscalling/downscaling
	- llama: 3
- questions
	- single node multiple GPU or multiple nodes
	  - single node
	- what scheme to analysis? Ring attention?
	- Why not consider the bandwidth in the question? It the max equal to minimum time?
- arbitrary block sizes
- extend ring scheme to MLP
- communication volume
- shard sequence/maximum sequence


![2024-11-06-fig1](/Users/terryxu/Study/2024-Fall/DPHPC/project/dphpc-2024-fall/figs/2024-11-06-fig1.jpg)

![2024-11-06-fig2](/Users/terryxu/Study/2024-Fall/DPHPC/project/dphpc-2024-fall/figs/2024-11-06-fig2.jpg)
