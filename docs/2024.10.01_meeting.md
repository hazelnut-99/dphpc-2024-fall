#### Main Topics
tensor parallelism and sequence parallelism </br>
matrix multiplication in MLP and attention layers
- slicing matrix
- overlapping computation and communication


#### Papers
Megatron-LM: https://arxiv.org/pdf/1909.08053  </br>
DeepSpeed Ulysses: https://arxiv.org/pdf/2309.14509   </br>
REDUCING ACTIVATION RECOMPUTATION IN LARGE TRANSFORMER  </br>
MODELS: https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf  </br>
ring attention: [todo]

#### Systems
nanotron: https://github.com/huggingface/nanotron </br>
pytorch distributed  </br>


#### misc.
focus on training not inference (inference is more complicated due to kv-cache)


![image](https://github.com/user-attachments/assets/a36961d9-9a26-4bed-b041-9eabc8c3798f)
![image](https://github.com/user-attachments/assets/061061ed-1a1e-402a-acde-2bd8a0b26e8b)
